{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#Implement the sigmoid activation \n",
        "#function and it's derivative.\n",
        "# here we compute the sigmoid nonlinearity \n",
        "def sigmoid(x, deriv=False):\n",
        "  output = 1/(1+np.exp(-x))\n",
        "  return output\n",
        "\n",
        "# convert output of sigmoid function to its derivative\n",
        "def sigmoid_output_derivative(output):\n",
        "  return output*(1-output)\n",
        "\n",
        "# Random input and output.  \n",
        "X = np.array([[0,0,1],\n",
        "              [0,1,1],\n",
        "              [1,0,1],\n",
        "              [1,1,1]])\n",
        "                \n",
        "y = np.array([[0],\n",
        "\t\t\t  [1],\n",
        "\t\t\t  [1],\n",
        "\t\t\t  [0]])\n",
        "\n",
        "# Set a numpy seed\n",
        "np.random.rand()\n",
        "# Randomly initialize our weights with mean 0\n",
        "# Keep in the mind the dimensions!\n",
        "w0 = 2*np.random.random((3,4)) - 1\n",
        "w1 = 2*np.random.random((4,1)) - 1\n",
        "\n",
        "# Repeat until slope is 0\n",
        "for j in range(60000):\n",
        "\n",
        "\t# Feed forward. Fill in the missing operation in sigmoid()\n",
        "    # Remember, we are doing w * X + b\n",
        "    # Feed forward through layers 0, 1, and 2\n",
        "    l0 = X\n",
        "    l1 = sigmoid(np.dot(l0,w0))\n",
        "    l2 = sigmoid(np.dot(l1,w1))\n",
        "\n",
        "    # Compute the error: true - prediction\n",
        "    # how much did we miss the target value?\n",
        "    l2_error = l2 - y\n",
        "    \n",
        "    if (j % 10000) == 0:\n",
        "        print (\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
        "        \n",
        "    # We want to compute the gradients. Recall the output\n",
        "    # layer partial derivatives. What does the error tell us?\n",
        "    # Fill in the missing right hand side.\n",
        "\n",
        "    # In what direction did the change happen, did we go in the right one? If we are not sure then dont change too much\n",
        "    # We calculate teh slope at curent position\n",
        "    l2_delta = l2_error * sigmoid_output_derivative(l2)\n",
        "\n",
        "    # We want to know how much did layer1 contridube \n",
        "    # to the error of layer2. Fill in the missing operation.\n",
        "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
        "    l1_error = l2_delta.dot(w1.T)\n",
        "    \n",
        "    # Same as above, compute the partial derivatives of\n",
        "    # layer1 now just as in layer2.\n",
        "    # In what direction did the change happen, did we go in the right one? If we are not sure then dont change too much\n",
        "    l1_delta = l1_error * sigmoid_output_derivative(l1)\n",
        "\n",
        "    # Update the weights\n",
        "    # Change the x by the negative of the slope\n",
        "    w1 += l1.T.dot(l2_delta)\n",
        "    w0 += l0.T.dot(l1_delta)"
      ],
      "metadata": {
        "id": "jWcLRmKXQWpT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a50f31-0327-4091-855c-3c14c14aabbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error:0.5008152472352528\n",
            "Error:0.5000073609594783\n",
            "Error:0.5000039139453416\n",
            "Error:0.5000026953525831\n",
            "Error:0.5000020654158729\n",
            "Error:0.500001678660582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And with an extremely large alpha, we see a textbook example of divergence, with the error increasing instead of decreasing. Hardlining at 0.5."
      ],
      "metadata": {
        "id": "GxME5O-S4cmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference:\n",
        "\n",
        "\n",
        "1.   https://towardsdatascience.com/a-step-by-step-implementation-of-gradient-descent-and-backpropagation-d58bda486110\n",
        "2.   https://github.com/peterroelants/peterroelants.github.io/blob/main/notebooks/neural_net_implementation/neural-network-implementation-part01.ipynb\n",
        "3. https://blog.paperspace.com/part-1-generic-python-implementation-of-gradient-descent-for-nn-optimization/\n",
        "4. https://iamtrask.github.io/2015/07/27/python-network-part2/\n",
        "\n"
      ],
      "metadata": {
        "id": "DpfFjrHO5B8L"
      }
    }
  ]
}